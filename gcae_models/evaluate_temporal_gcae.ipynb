{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cddbb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "from gcae_tcn import GVPEncoder, GVPDecoder, TCNModel\n",
    "from gcae_transformer import TransformerEncoder\n",
    "import matplotlib.pyplot as plt \n",
    "import mdtraj as md \n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Batch\n",
    "from tqdm import tqdm \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE \n",
    "\n",
    "import umap\n",
    "import math\n",
    "\n",
    "from functools import partial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412b5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    elif torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c54a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",   # or \"sans-serif\"\n",
    "    \"font.size\": 14,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"axes.titlesize\": 16,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35974d7d",
   "metadata": {},
   "source": [
    "# Load in latent from path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db4e69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "split= False\n",
    "protein = 'chignolin' #'pentapeptide' #'chignolin' # or, 'fs_peptide'\n",
    "if protein == 'chignolin':\n",
    "    pdb,files = dataset.load_DESRES(protein, data_dir='./', analyze=True, plot=False)\n",
    "    test_traj_num = 26\n",
    "    print(files[test_traj_num])\n",
    "    \n",
    "\n",
    "elif protein == 'pentapeptide':\n",
    "    pdb,files = dataset.load_pentapeptide(data_dir='pentapeptide/')\n",
    "    files = dataset.reshape_time_window(files, pdb, num_files=25, traj_len=5001, num_split=1)\n",
    "    test_traj_num = 12\n",
    "    print(files[test_traj_num])\n",
    "    split=True\n",
    "\n",
    "elif protein == 'fs_peptide':\n",
    "    pdb, files = dataset.load_fs_peptide(data_dir='fs_peptide/')\n",
    "    files = dataset.reshape_time_window(files, pdb, num_files=28, traj_len=10000, num_split=1)\n",
    "    test_traj_num = 12\n",
    "    print(files[test_traj_num])\n",
    "    split=True\n",
    "\n",
    "\n",
    "file = files[test_traj_num]\n",
    "file0 = files[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'gcae_experiments/more_recent_models/transformer/chignolin/Falsenorm_10smoothing_5000ep_0.0005lr_0.1dr_3layers_5edge_32latentdim_1e-06reg_1e-06temp_4nheads_2encoderlayers_512maxseqlen_CAUSAL/'\n",
    "model_num_load=2649\n",
    "latent_dim = 32\n",
    "temporal = 'transformer'\n",
    "normalize=False\n",
    "\n",
    "top_k = 10\n",
    "n_layers=3\n",
    "dr = 0.1\n",
    "num_layers = 3\n",
    "lr = 5e-4\n",
    "smooth = 10\n",
    "\n",
    "T = 512\n",
    "n_heads = 4\n",
    "n_encoder_layers = 2\n",
    "lambda_reg = 0\n",
    "lambda_temp = 0\n",
    "\n",
    "kernel = 5\n",
    "layers = [64,64,64]\n",
    "\n",
    "\n",
    "if files[0][-4:] == '.xtc':\n",
    "    traj0 = md.load_xtc(files[0], top=pdb)\n",
    "else:\n",
    "    traj0 = md.load_dcd(files[0], top=pdb)\n",
    "topology = md.load(pdb).topology\n",
    "\n",
    "\n",
    "test_structures = dataset.generate_structures([files[test_traj_num]], pdb, traj0, split=split, smooth=smooth, normalize=normalize)\n",
    "test_frame_dataset = dataset.LigandDataset(test_structures, top_k=top_k)\n",
    "test_seq_dataset = dataset.SequenceDataset([len(traj0)], sequence_length = T, stride=T, include_partial=True)\n",
    "\n",
    "\n",
    "\n",
    "device = determine_device()\n",
    "print(f\"On device: {device}\")\n",
    "\n",
    "node_h_dim = (100, 16)\n",
    "edge_h_dim = (32, 1)\n",
    "node_num = md.load(pdb).topology.n_residues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f82696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_sparse(batch, frame_dataset):\n",
    "    # 'batch' is a list of tuples: [(start0, end0), (start1, end1), ...]\n",
    "    frames, seq_ptr = [], [0]\n",
    "\n",
    "    for s, e in batch:\n",
    "        frames.extend(frame_dataset[i] for i in range(s, e))\n",
    "        seq_ptr.append(len(frames))\n",
    "\n",
    "    # This is the key line â€” returns a PyG Batch\n",
    "    out = Batch.from_data_list(frames)\n",
    "    out.seq_ptr = torch.tensor(seq_ptr, dtype=torch.long)\n",
    "    out.seq_len = e - s\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89aa8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_seq_dataset,\n",
    "                         batch_size=1,\n",
    "                         shuffle=False,\n",
    "                         collate_fn = partial(collate_sparse, frame_dataset=test_frame_dataset),\n",
    "                         num_workers=0,\n",
    "                         drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = GVPEncoder((6,3), node_h_dim, (32,1), edge_h_dim,\n",
    "                        latent_dim=latent_dim,\n",
    "                        n_layers= n_layers,\n",
    "                        drop_rate= dr,\n",
    "                        node_num=node_num).to(device)\n",
    "\n",
    "decoder = GVPDecoder((6,3), node_h_dim, (32,1), edge_h_dim,\n",
    "                        latent_dim=latent_dim,\n",
    "                        n_layers= n_layers,\n",
    "                        drop_rate= dr, dense_mode=True, node_num=node_num).to(device)\n",
    "\n",
    "tcn = TCNModel(input_size=latent_dim, channel_size=layers, input_length=512, kernel_size=kernel).to(device)\n",
    "transformer = TransformerEncoder(latent_dim, max_seq_len=T, nhead=n_heads,num_layers=n_encoder_layers, dropout=dr)\n",
    "\n",
    "if temporal == 'tcn':\n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()) + list(tcn.parameters()), lr=lr)\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()) + list(transformer.parameters()), lr=lr)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "encoder_state_dict = torch.load(os.path.join(path, f'encoder_epoch-{model_num_load}.pt'), map_location=device, weights_only=True)\n",
    "encoder.load_state_dict(encoder_state_dict)\n",
    "encoder.to(device)\n",
    "\n",
    "decoder_state_dict = torch.load(os.path.join(path, f'decoder_epoch-{model_num_load}.pt'), map_location=device, weights_only=True)\n",
    "decoder.load_state_dict(decoder_state_dict)\n",
    "decoder.to(device)\n",
    "if temporal == 'tcn':\n",
    "    tcn_state_dict = torch.load(os.path.join(path, f'tcn_epoch-{model_num_load}.pt'), map_location=device, weights_only=True)\n",
    "    tcn.load_state_dict(tcn_state_dict)\n",
    "    tcn.to(device)\n",
    "else: \n",
    "    transformer_state_dict = torch.load(os.path.join(path, f'transformer_epoch-{model_num_load}.pt'), map_location=device, weights_only=True)\n",
    "    transformer.load_state_dict(transformer_state_dict)\n",
    "    transformer.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = torch.load(os.path.join(path, 'losses.pt'), map_location='cpu')\n",
    "\n",
    "losses_values = list(losses.values())\n",
    "loss = losses_values#[loss[0] for loss in losses_values]\n",
    "# loss = losses_values\n",
    "plt.plot(range(len(loss)), loss)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04d4b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "if temporal == 'tcn':\n",
    "    tcn.eval()\n",
    "else:\n",
    "    transformer.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "all_mu = []\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        \n",
    "        batch = batch.to(device)\n",
    "        nodes = (batch.node_s, batch.node_v)\n",
    "        edges = (batch.edge_s, batch.edge_v)\n",
    "\n",
    "        GT = batch.x \n",
    "\n",
    "        z = encoder(nodes, batch.edge_index, edges)\n",
    "        z_seq = torch.stack([z[start:end] for start,end in zip(batch.seq_ptr[:-1], batch.seq_ptr[1:])])\n",
    "        if temporal == 'tcn':\n",
    "            z_seq_out = tcn(z_seq)\n",
    "        else:\n",
    "            z_seq_out = transformer(z_seq)\n",
    "        z_out = torch.cat([seq for seq in z_seq_out], dim=0)\n",
    "        pred = decoder(z_out, batch.edge_index, edges)\n",
    "        reg = torch.mean(torch.norm(z, dim=1)**2)\n",
    "\n",
    "        temp = torch.mean((z[:, 1:] - z[:, :-1]) **2)\n",
    "        \n",
    "        loss_A = loss_function(GT, pred)\n",
    "        loss = loss_A + lambda_reg * reg + lambda_temp * temp #1e-6 * reg\n",
    "\n",
    "        all_mu.append(z_out.cpu())\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        \n",
    "latents = torch.cat(all_mu, dim=0)\n",
    "latent_variances = torch.var(latents, dim=0)\n",
    "test_loss /= len(test_loader)\n",
    "\n",
    "rmsd_angstroms = math.sqrt(test_loss) * 10\n",
    "\n",
    "print(f\"RMSD on test data {rmsd_angstroms:.8f} Ã…\")\n",
    "\n",
    "print(\"Per-latent-dimension variance across dataset:\")\n",
    "for i, var in enumerate(latent_variances):\n",
    "    print(f\"Dimension {i}: variance = {var.item():.4f}\")\n",
    "\n",
    "torch.save(latents, os.path.join(path, \"MORErecomputed_test_latents.pt\"))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b732eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca = PCA(n_components=2)\n",
    "latent_pca = pca.fit_transform(latents)\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=50, learning_rate='auto')\n",
    "latent_tsne = tsne.fit_transform(latents)\n",
    "\n",
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d7b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16,6), constrained_layout=True)\n",
    "\n",
    "\n",
    "ax1.hexbin(latent_pca[:,0], latent_pca[:,1], bins='log')\n",
    "ax1.set_title(\"PCA of Latent Codes\")\n",
    "ax1.set_xlabel(\"PC1\")\n",
    "ax1.set_ylabel(\"PC2\")\n",
    "\n",
    "ax2.scatter(latent_tsne[:,0], latent_tsne[:,1], alpha=0.5)\n",
    "ax2.set_title(\"t-SNE of Latent Codes\")\n",
    "ax2.set_xlabel(\"t-SNE dim 1\")\n",
    "ax2.set_ylabel(\"t-SNE dim 2\")\n",
    "\n",
    "ax3.scatter(embedding[:,0], embedding[:,1], alpha=0.5)\n",
    "ax3.set_title(\"UMAP of Latent Codes\")\n",
    "ax3.set_xlabel(\"UMAP dim 1\")\n",
    "ax3.set_ylabel(\"UMAP dim 2\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7950e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_model =  embedding #latent_pca #latent_tsne #embedding\n",
    "center = (0,7)\n",
    "x_radius = 1\n",
    "y_radius = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342bad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_points = []\n",
    "for i in range(dim_model.shape[0]):\n",
    "    if (center[0] - x_radius <= dim_model[i, 0]) and (dim_model[i,0] <= center[0] + x_radius):\n",
    "        if (center[1] - y_radius <= dim_model[i, 1]) and (dim_model[i,1] <= center[1] + y_radius):\n",
    "            valid_points.append(i)\n",
    "\n",
    "print(len(valid_points))\n",
    "\n",
    "print(valid_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a632f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_num=507\n",
    "if file[-4:] == '.xtc':\n",
    "    traj = md.load_xtc(file, top=pdb)\n",
    "    traj0 = md.load_xtc(file0, top=pdb)\n",
    "else:\n",
    "    traj = md.load_dcd(file, top=pdb)\n",
    "    traj0 = md.load_dcd(file0, top=pdb)\n",
    "\n",
    "traj.superpose(traj0, frame=0)\n",
    "traj.center_coordinates()\n",
    "\n",
    "rmsd = md.rmsd(traj, traj, frame=frame_num, precentered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bca2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16,6), constrained_layout=True)\n",
    "\n",
    "sc1 = ax1.scatter(latent_pca[:, 0], latent_pca[:, 1], c=rmsd, cmap='viridis', s=10)\n",
    "ax1.scatter(latent_pca[frame_num, 0], latent_pca[frame_num, 1], c='r', s=120,  marker='*')\n",
    "ax1.set_title(\"PCA of Latent Codes\")\n",
    "ax1.set_xlabel(\"PC1\")\n",
    "ax1.set_ylabel(\"PC2\")\n",
    "\n",
    "ax2.scatter(latent_tsne[:, 0], latent_tsne[:, 1], c=rmsd, cmap='viridis', s=10)\n",
    "ax2.scatter(latent_tsne[frame_num, 0], latent_tsne[frame_num, 1], c='r', s=120,  marker='*')\n",
    "ax2.set_title(\"t-SNE of Latent Codes\")\n",
    "ax2.set_xlabel(\"t-SNE dim 1\")\n",
    "ax2.set_ylabel(\"t-SNE dim 2\")\n",
    "\n",
    "ax3.scatter(embedding[:, 0], embedding[:, 1], c=rmsd, cmap='viridis', s=10)\n",
    "ax3.scatter(embedding[frame_num, 0], embedding[frame_num, 1], c='r', s=120,  marker='*')\n",
    "ax3.set_title(\"UMAP of Latent Codes\")\n",
    "ax3.set_xlabel(\"UMAP dim 1\")\n",
    "ax3.set_ylabel(\"UMAP dim 2\")\n",
    "\n",
    "cbar = f.colorbar(sc1, ax=[ax1, ax2, ax3], orientation=\"vertical\")\n",
    "cbar.set_label(\"RMSD\")\n",
    "\n",
    "#plt.show()\n",
    "plt.savefig(f'/Users/lreeder/Documents/Stanford/5fifth_year/thesis/figs/{protein}/{temporal}_rmsd{frame_num}.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502582cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15edfa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = latent_pca[:,0]\n",
    "y = latent_pca[:,1]\n",
    "starting_frame = 0\n",
    "animation_len =  100\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "line, = ax.plot([], [], lw=2, color='red')\n",
    "point, = ax.plot([], [], 'ro')\n",
    "ax.hexbin(latent_pca[:,0], latent_pca[:,1], bins='log')\n",
    "#ax.hexbin(latent_pca[:,0], latent_pca[:,1], bins='log')\n",
    "ax.set_xlim(x.min() - 0.01, x.max() + 0.01)\n",
    "ax.set_ylim(y.min() - 0.01, y.max() + 0.01)\n",
    "ax.set_title(\"2D Trajectory over time - latent dimension 1 and 2\")\n",
    "ax.set_xlabel(\"Latent dim 1\")\n",
    "ax.set_ylabel(\"Latent dim 2\")\n",
    "ax.grid(True)\n",
    "ax.set_aspect('equal')\n",
    "timestamp = ax.text(0.02, 0.95, '', transform=ax.transAxes, fontsize=12, ha='left', va='top')\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    point.set_data([], [])\n",
    "    return line, point\n",
    "\n",
    "def old_update(frame):\n",
    "    if frame < animation_len:\n",
    "        line.set_data(x[starting_frame:frame+1], y[starting_frame:frame+1])\n",
    "        point.set_data(x[starting_frame + frame], y[starting_frame + frame])\n",
    "        timestamp.set_text(f'Time step: {starting_frame + frame}')\n",
    "    else:\n",
    "        line.set_data(x, y)\n",
    "        point.set_data([x[starting_frame + animation_len-1]], [y[starting_frame + animation_len-1]])\n",
    "        timestamp.set_text(f'Time step: {starting_frame + animation_len-1}')\n",
    "    return line, point, timestamp\n",
    "\n",
    "def update(frame):\n",
    "    line.set_data(x[starting_frame:starting_frame +frame+1], y[starting_frame:starting_frame +frame+1])\n",
    "    point.set_data([x[starting_frame + frame]], [y[starting_frame + frame]])\n",
    "    return line, point\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, frames=animation_len, init_func=init, interval=200, blit=True)\n",
    "HTML(ani.to_jshtml())\n",
    "# or for a GIF:\n",
    "#ani.save('trajectory.gif', writer='pillow', fps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33262573",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,1, figsize=(10,5))\n",
    "animation_len = 200\n",
    "axs[0].plot(range(animation_len), x[starting_frame:starting_frame + animation_len], color=\"darkred\", linewidth=2)\n",
    "axs[0].set_title(\"PC 1\")\n",
    "axs[0].set_xlabel(\"t\")\n",
    "axs[0].spines['top'].set_visible(False)\n",
    "axs[0].spines['right'].set_visible(False)\n",
    "axs[0].grid(alpha=0.3)\n",
    "\n",
    "axs[1].plot(range(animation_len), y[starting_frame:starting_frame + animation_len], color='darkred', linewidth=2)\n",
    "axs[1].set_title(\"PC 2\")\n",
    "axs[1].set_xlabel(\"t\")\n",
    "axs[1].spines['top'].set_visible(False)\n",
    "axs[1].spines['right'].set_visible(False)\n",
    "axs[1].grid(alpha=0.3)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b7ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31d7000c",
   "metadata": {},
   "source": [
    "# Autocorrelation Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211264e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_time_path = '/Users/lreeder/Documents/Stanford/research/gnn_md/redo_TICA/new_gvp_training/'\n",
    "\n",
    "no_time_latents = torch.load(os.path.join(no_time_path, 'inference_latents.pt'))\n",
    "\n",
    "\n",
    "some_time_path = 'more_recent_models/no_time/pentapeptide/nonormal_10smoothing_1000ep_0.0005lr_0.1dr_3layers_10edge_12latentdim_1e-06reg_0.0temp/'\n",
    "some_time_latents = torch.load(os.path.join(some_time_path, 'test_latents.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee302549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "def latent_acf(latents, max_lag=200, demean=True):\n",
    "    \"\"\"\n",
    "    latents: array (T, d) or list of arrays per-trajectory [(T1,d), (T2,d), ...]\n",
    "    returns: mean_acf (max_lag+1,), acfs_per_dim (d, max_lag+1)\n",
    "    \"\"\"\n",
    "    # If you have multiple independent trajectories, compute per-trajectory ACF and average.\n",
    "    if isinstance(latents, list):\n",
    "        acf_stack = []\n",
    "        for Z in latents:\n",
    "            if demean:\n",
    "                Z = Z - Z.mean(axis=0, keepdims=True)\n",
    "            per_dim = [acf(Z[:, i], nlags=max_lag, fft=True) for i in range(Z.shape[1])]\n",
    "            acf_stack.append(np.vstack(per_dim))           # (d, max_lag+1)\n",
    "        acf_stack = np.stack(acf_stack, axis=0)            # (n_traj, d, max_lag+1)\n",
    "        acfs = acf_stack.mean(axis=0)                      # average over trajectories -> (d, max_lag+1)\n",
    "    else:\n",
    "        Z = latents - latents.mean(axis=0, keepdims=True) if demean else latents\n",
    "        acfs = np.vstack([acf(Z[:, i], nlags=max_lag, fft=True) for i in range(Z.shape[1])])\n",
    "\n",
    "    mean_acf = acfs.mean(axis=0)\n",
    "    return mean_acf, acfs\n",
    "\n",
    "def plot_acf(mean_acf, acfs=None, label=None, show_dims=False):\n",
    "    lags = np.arange(len(mean_acf))\n",
    "    plt.figure(figsize=(6,4))\n",
    "    if show_dims and acfs is not None:\n",
    "        for i in range(acfs.shape[0]):\n",
    "            plt.plot(lags, acfs[i], alpha=0.15, linewidth=1)\n",
    "    plt.plot(lags, mean_acf, linewidth=2, label=label or \"mean ACF\", color='darkred')\n",
    "    plt.axhline(0, linestyle=\"--\", linewidth=1)\n",
    "    plt.xlabel(\"Lag (frames)\")\n",
    "    plt.ylabel(\"Autocorrelation\")\n",
    "    if label:\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d7acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acf_base, acfs_base = latent_acf(no_time_latents, max_lag=300)\n",
    "mean_acf_seq,  acfs_seq = latent_acf(latents,  max_lag=300)\n",
    "mean_acf_some_time, acfs_some = latent_acf(some_time_latents, max_lag=300)\n",
    "colors = [\"#0072B2\",\"#E69F00\", 'darkred']\n",
    "lags = np.arange(len(mean_acf_base))\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(lags, mean_acf_base, label=\"Individual frames\", linewidth=2, c=colors[0])\n",
    "plt.plot(lags, mean_acf_some_time, label=\"Implicit time\", linewidth=2, c=colors[1])\n",
    "plt.plot(lags, mean_acf_seq,  label=\"Explicit sequence model\", linewidth=2, c='darkred')\n",
    "plt.axhline(0, linestyle=\"--\", linewidth=1)\n",
    "plt.xlabel(\"Lag (frames)\")\n",
    "plt.ylabel(\"Autocorrelation\")\n",
    "plt.legend()\n",
    "plt.savefig(f'/Users/lreeder/Documents/Stanford/5fifth_year/thesis/figs/{protein}/{temporal}_ACFvsLag.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e62238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_time(mean_acf, max_fit_lag=100):\n",
    "    y = mean_acf[1:max_fit_lag+1]\n",
    "    lags = np.arange(1, max_fit_lag+1)\n",
    "    # Use only positive ACF to avoid log of nonpositive\n",
    "    mask = y > 0\n",
    "    lags, y = lags[mask], y[mask]\n",
    "    if len(y) < 5:\n",
    "        return np.nan\n",
    "    # Fit log y â‰ˆ -lags/Ï„ + c  â†’ Ï„ = -1 / slope\n",
    "    slope, intercept = np.polyfit(lags, np.log(y), 1)\n",
    "    tau = -1.0 / slope if slope < 0 else np.nan\n",
    "    return tau\n",
    "\n",
    "tau_base = correlation_time(mean_acf_base, max_fit_lag=300)\n",
    "tau_some = correlation_time(mean_acf_some_time, max_fit_lag=300)\n",
    "tau_seq  = correlation_time(mean_acf_seq,  max_fit_lag=300)\n",
    "print(f\"Ï„_c (no sequence): {tau_base:.1f} frames\")\n",
    "print(f\"Ï„_c (some_time):    {tau_some:.1f} frames\")\n",
    "print(f\"Ï„_c (sequence):    {tau_seq:.1f} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea40f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "def acf_per_dim(latents, max_lag=300, demean=True):\n",
    "    \"\"\"\n",
    "    latents: (T, d) array OR list of arrays [(T1,d), (T2,d), ...]\n",
    "    returns:\n",
    "      acf_dims: (d, max_lag+1)  # mean ACF per dimension, averaged over trajectories\n",
    "      acf_dims_all: list of arrays length n_traj, each (d, max_lag+1)\n",
    "    \"\"\"\n",
    "    # Gather trajectories as a list\n",
    "    Z_list = latents if isinstance(latents, list) else [latents]\n",
    "    # Sanity: all have same d\n",
    "    d = Z_list[0].shape[1]\n",
    "    acf_dims_all = []\n",
    "    for Z in Z_list:\n",
    "        if demean:\n",
    "            Z = Z - Z.mean(axis=0, keepdims=True)\n",
    "        acfs = np.vstack([acf(Z[:, i], nlags=max_lag, fft=True) for i in range(d)])  # (d, L)\n",
    "        acf_dims_all.append(acfs)\n",
    "    acf_dims = np.mean(np.stack(acf_dims_all, axis=0), axis=0)  # (d, L)\n",
    "    return acf_dims, acf_dims_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e48f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_acf_base, acfs_base = acf_per_dim(no_time_latents, max_lag=300)\n",
    "dim_acf_seq,  acfs_seq =acf_per_dim(latents,  max_lag=300)\n",
    "dim_acf_some_time, acfs_some = acf_per_dim(some_time_latents, max_lag=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c93c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def plot_acf_per_dim(acf_dims, max_cols=4, cutoff=None, title=None):\n",
    "    \"\"\"\n",
    "    acf_dims: (d, L)   from acf_per_dim\n",
    "    cutoff: truncate lags to show (e.g., 100); None -> full length\n",
    "    \"\"\"\n",
    "    d, L = acf_dims.shape\n",
    "    lags = np.arange(L)\n",
    "    if cutoff is None: cutoff = L\n",
    "    rows = math.ceil(d / max_cols)\n",
    "    cols = min(d, max_cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(3.3*cols, 2.8*rows), squeeze=False)\n",
    "    idx = 0\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            ax = axes[r, c]\n",
    "            if idx < d:\n",
    "                ax.plot(lags[:cutoff], acf_dims[idx, :cutoff], linewidth=1.8)\n",
    "                ax.set_title(f\"dim {idx}\")\n",
    "                ax.set_xlim(0, cutoff-1)\n",
    "                ax.set_ylim(max(1e-5, acf_dims[:,1:cutoff].min()), 1.0)\n",
    "                if r == rows-1: ax.set_xlabel(\"Lag (frames)\")\n",
    "                if c == 0:      ax.set_ylabel(\"Autocorr (log)\")\n",
    "            else:\n",
    "                ax.axis(\"off\")\n",
    "            idx += 1\n",
    "    if title: fig.suptitle(title, y=1.02)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df98024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "acf_dims_no, _  = acf_per_dim(no_time_latents,  max_lag=300)\n",
    "acf_dims_imp, _ = acf_per_dim(some_time_latents, max_lag=300)\n",
    "acf_dims_seq, _ = acf_per_dim(latents,      max_lag=300)\n",
    "\n",
    "plot_acf_per_dim(acf_dims_no,  cutoff=100, title=\"No sequence: per-dim ACF\")\n",
    "plot_acf_per_dim(acf_dims_imp, cutoff=100, title=\"Implicit time: per-dim ACF\")\n",
    "plot_acf_per_dim(acf_dims_seq, cutoff=100, title=\"Sequence: per-dim ACF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee029c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_acf(acf_dims, cutoff=None, title=None):\n",
    "    d, L = acf_dims.shape\n",
    "    if cutoff is None: cutoff = L\n",
    "    # Use log scale values but keep zeros positive\n",
    "    A = np.clip(acf_dims[:, :cutoff], 1e-6, 1.0)\n",
    "    fig, ax = plt.subplots(figsize=(8, max(2.5, d*0.25)))\n",
    "    im = ax.imshow(A, aspect='auto', origin='lower')\n",
    "    ax.set_xlabel(\"Lag (frames)\")\n",
    "    ax.set_ylabel(\"Latent dimension\")\n",
    "    ax.set_title(title or \"ACF heatmap (clip to [1e-6, 1])\")\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label(\"Autocorrelation\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'/Users/lreeder/Documents/Stanford/5fifth_year/thesis/figs/{protein}/{temporal}_ACFdim{d}.pdf', bbox_inches='tight')\n",
    "    #plt.show()\n",
    "\n",
    "heatmap_acf(acf_dims_imp, cutoff=150, title=\"Implicit time: ACF heatmap\")\n",
    "heatmap_acf(acf_dims_seq, cutoff=150, title=\"Sequence: ACF heatmap\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6090cc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gvp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
