{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c489d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../gcae_models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7181ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "from gcae_tcn import GVPEncoder, GVPDecoder, TCNModel\n",
    "from gcae_transformer import TransformerEncoder\n",
    "import matplotlib.pyplot as plt \n",
    "import mdtraj as md \n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Batch\n",
    "from tqdm import tqdm \n",
    "from torch.utils.data import DataLoader\n",
    "import torch_cluster\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE \n",
    "\n",
    "import umap\n",
    "import math\n",
    "\n",
    "from functools import partial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27425901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",   # or \"sans-serif\"\n",
    "    \"font.size\": 14,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"axes.titlesize\": 16,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12\n",
    "})\n",
    "\n",
    "colors = [\"#0072B2\",\"#E69F00\", 'darkred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f5a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forecasting_main import Config, Trainer\n",
    "from forecasting_utils import get_forecasting_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb911858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    elif torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2034691e",
   "metadata": {},
   "source": [
    "Load in Forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25a376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'latent_traj'\n",
    "data_path = 'more_recent_models/tcn/pentapeptide/Falsenorm_10smoothing_2000ep_0.0005lr_0.1dr_2batch_3layers_10edge_16latentdim_0.0reg_0.0temp_TCN3layers_TCN5kernel/'\n",
    "latent_filename = 'MORErecomputed_test_latents.pt'\n",
    "time_lag = 1\n",
    "stride = 1\n",
    "latent_dim = 16\n",
    "hidden_dim = 512\n",
    "num_layers = 5\n",
    "random_time_sampling = False\n",
    "shuffle_batch = False\n",
    "load_path = os.path.join(data_path, f'forecast_ckpts/latest_model_{time_lag}lag_{stride}stride.pt')\n",
    "sigma_coef = 1.0\n",
    "beta_fn = 't^2'\n",
    "debug = False\n",
    "sample_only = True\n",
    "overfit = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec60d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if dataset == 'latent_traj':\n",
    "conf = Config(\n",
    "    dataset=dataset,\n",
    "    debug=debug,\n",
    "    overfit=overfit,\n",
    "    sigma_coef=sigma_coef,\n",
    "    beta_fn=beta_fn,\n",
    "    latent_dim=latent_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    data_path=data_path,\n",
    "    latent_filename=latent_filename,\n",
    "    time_lag=time_lag,\n",
    "    stride=stride,\n",
    "    random_time_sampling=random_time_sampling,\n",
    "    shuffle_batch=shuffle_batch\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    conf, \n",
    "    load_path = load_path,\n",
    "    sample_only = sample_only\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812c98e1",
   "metadata": {},
   "source": [
    "Load in autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69abd97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = data_path\n",
    "temporal_model = 'tcn'\n",
    "model_num_load = 1999\n",
    "latent_dim = 16\n",
    "\n",
    "top_k = 10\n",
    "dr = 0.1\n",
    "n_layers = 3\n",
    "lr = 0.0005\n",
    "\n",
    "T = 512\n",
    "n_heads = 4\n",
    "n_encoder_layers = 2\n",
    "channel_size= [64,64,64]\n",
    "kernel_size = 5\n",
    "lambda_reg = 0\n",
    "lambda_temp = 0\n",
    "\n",
    "device = determine_device()\n",
    "\n",
    "\n",
    "test_file = 'pentapeptide/split_25files_5001len_1chunks/file20_part0.xtc'\n",
    "traj0_file = 'pentapeptide/split_25files_5001len_1chunks/file0_part0.xtc'\n",
    "pdb = 'pentapeptide/pentapeptide-impl-solv.pdb'\n",
    "if test_file[-4:] == '.xtc':\n",
    "    traj0 = md.load_xtc(traj0_file, top=pdb)\n",
    "else:\n",
    "    traj0 = md.load_dcd(traj0_file, top=pdb)\n",
    "\n",
    "test_structures = dataset.generate_structures([test_file], pdb, traj0, split=True, smooth=10)\n",
    "test_frame_dataset = dataset.LigandDataset(test_structures, top_k=top_k)\n",
    "test_seq_dataset = dataset.SequenceDataset([len(traj0)] , sequence_length=T, stride=T, include_partial=True)\n",
    "\n",
    "\n",
    "node_h_dim = (100, 16)\n",
    "edge_h_dim = (32, 1)\n",
    "node_num = md.load(pdb).topology.n_residues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d07426",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_num = 4340\n",
    "if test_file[-4:] == '.xtc':\n",
    "    traj = md.load_xtc(test_file, top=pdb)\n",
    "else:\n",
    "    traj = md.load_dcd(test_file, top=pdb)\n",
    "\n",
    "traj.superpose(traj0, frame=0)\n",
    "traj.center_coordinates()\n",
    "\n",
    "rmsd = md.rmsd(traj, traj, frame=frame_num, precentered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a080d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_sparse(batch, frame_dataset):\n",
    "    # 'batch' is a list of tuples: [(start0, end0), (start1, end1), ...]\n",
    "    frames, seq_ptr = [], [0]\n",
    "\n",
    "    for s, e in batch:\n",
    "        frames.extend(frame_dataset[i] for i in range(s, e))\n",
    "        seq_ptr.append(len(frames))\n",
    "\n",
    "    out = Batch.from_data_list(frames)\n",
    "    out.seq_ptr = torch.tensor(seq_ptr, dtype=torch.long)\n",
    "    out.seq_len = e - s\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f102a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_seq_dataset,\n",
    "                         batch_size=1,\n",
    "                         shuffle=False,\n",
    "                         collate_fn = partial(collate_sparse, frame_dataset=test_frame_dataset),\n",
    "                         num_workers=0,\n",
    "                         drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778786cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = GVPEncoder((6,3), node_h_dim, (32,1), edge_h_dim,\n",
    "                        latent_dim=latent_dim,\n",
    "                        n_layers= n_layers,\n",
    "                        drop_rate= dr,\n",
    "                        node_num=node_num).to(device)\n",
    "\n",
    "decoder = GVPDecoder((6,3), node_h_dim, (32,1), edge_h_dim,\n",
    "                        latent_dim=latent_dim,\n",
    "                        n_layers= n_layers,\n",
    "                        drop_rate= dr, dense_mode=True, node_num=node_num).to(device)\n",
    "\n",
    "tcn = TCNModel(input_size=latent_dim, channel_size=channel_size, input_length=512, kernel_size=kernel_size).to(device)\n",
    "transformer = TransformerEncoder(latent_dim, max_seq_len=T, nhead=n_heads,num_layers=n_encoder_layers, dropout=dr).to(device)\n",
    "\n",
    "if temporal_model == 'tcn':\n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()) + list(tcn.parameters()), lr=lr)\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()) + list(transformer.parameters()), lr=lr)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "encoder_state_dict = torch.load(os.path.join(model_path, f'encoder_epoch-{model_num_load}.pt'), map_location=device, weights_only=True)\n",
    "encoder.load_state_dict(encoder_state_dict)\n",
    "encoder.to(device)\n",
    "\n",
    "decoder_state_dict = torch.load(os.path.join(model_path, f'decoder_epoch-{model_num_load}.pt'), map_location=device, weights_only=True)\n",
    "decoder.load_state_dict(decoder_state_dict)\n",
    "decoder.to(device)\n",
    "if temporal_model == 'tcn':\n",
    "    tcn_state_dict = torch.load(os.path.join(model_path, f'tcn_epoch-{model_num_load}.pt'), map_location=device, weights_only=True)\n",
    "    tcn.load_state_dict(tcn_state_dict)\n",
    "    tcn.to(device)\n",
    "else: \n",
    "    transformer_state_dict = torch.load(os.path.join(model_path, f'transformer_epoch-{model_num_load}.pt'), map_location=device, weights_only=True)\n",
    "    transformer.load_state_dict(transformer_state_dict)\n",
    "    transformer.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f5b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_fns = {\n",
    "    'g_sigma' : None,\n",
    "    'g_other' : lambda t: sigma_coef * trainer.wide(1-t).pow(4),\n",
    "}\n",
    "\n",
    "k = 'g_sigma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a9eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d(base_coords, forecasted_coords, true_coords, title=None, ax=None):\n",
    "    base_coords = base_coords.detach().cpu().numpy()\n",
    "    forecasted_coords = forecasted_coords.detach().cpu().numpy()\n",
    "    true_coords = true_coords.detach().cpu().numpy()\n",
    "    bx,by,bz = base_coords[:,0], base_coords[:,1], base_coords[:,2]\n",
    "    fx,fy,fz = forecasted_coords[:,0], forecasted_coords[:,1], forecasted_coords[:,2]\n",
    "    tx,ty,tz = true_coords[:,0], true_coords[:,1], true_coords[:,2]\n",
    "    if ax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    ax.plot(bx, by, bz, marker='o', linestyle=':', color=colors[0], label='z0')\n",
    "    ax.scatter(bx, by, bz, c=colors[0], s=30)\n",
    "\n",
    "    ax.plot(fx, fy, fz, marker='o', linestyle='-', color=colors[1], label=\"Forecasted\")\n",
    "    ax.scatter(fx, fy, fz, c=colors[1], s=30)\n",
    "\n",
    "    ax.plot(tx, ty, tz, marker='o', linestyle='--', color=colors[2], label=\"z1\")\n",
    "    ax.scatter(tx, ty, tz, c=colors[2], s=30)\n",
    "\n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('z')\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eff346",
   "metadata": {},
   "outputs": [],
   "source": [
    "z0s, samples, z1s = trainer.test(diffusion_fns[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e46462",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_latents = torch.load(os.path.join(data_path, latent_filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e2e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_load_latents=load_latents[int(0.8*load_latents.shape[0]):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df00fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(end_load_latents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667459a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "z0s = z0s.to(device)\n",
    "z1s = z1s.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c8375",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = end_load_latents[0,:]\n",
    "cond = torch.unsqueeze(cond, dim=0)\n",
    "print(cond.shape)\n",
    "cond = cond.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed93d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_len = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d219c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chained_samples = []\n",
    "for i in range(sample_len):\n",
    "    cond = trainer.EM(cond, cond.to(device), diffusion_fn=diffusion_fns[k], return_avg=False)\n",
    "    chained_samples.append(cond)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e0149",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_latents = []\n",
    "for i in range(sample_len):\n",
    "    start = end_load_latents[i,:]\n",
    "    start = torch.unsqueeze(start, dim=0)\n",
    "    og_latents.append(start)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17ec332",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = int(0.8*load_latents.shape[0])\n",
    "first_forecasted_index = start_index + 1\n",
    "end_forecasted_index = first_forecasted_index + sample_len\n",
    "forecasted_indices = list(range(first_forecasted_index, end_forecasted_index))\n",
    "print(len(forecasted_indices))\n",
    "assert( len(forecasted_indices) == sample_len)\n",
    "print(\"first index: \", forecasted_indices[0], \"last index: \", forecasted_indices[-1])\n",
    "\n",
    "forecasted_latents = {idx: z.to('cpu') for idx, z in zip(forecasted_indices, chained_samples)}\n",
    "og_latents = {idx-1: z.to('cpu') for idx, z in zip(forecasted_indices, og_latents)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b748207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "if temporal_model == 'transformer':\n",
    "    transformer = transformer.to(device)\n",
    "else:\n",
    "    tcn = tcn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beb20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def _normalize(tensor, dim=-1):\n",
    "    '''\n",
    "    Normalizes a `torch.Tensor` along dimension `dim` without `nan`s.\n",
    "    '''\n",
    "    return torch.nan_to_num(\n",
    "        torch.div(tensor, torch.norm(tensor, dim=dim, keepdim=True)))\n",
    "\n",
    "\n",
    "def _rbf(D, D_min=0., D_max=20., D_count=16, device='cpu'):\n",
    "    '''\n",
    "    From https://github.com/jingraham/neurips19-graph-protein-design\n",
    "    \n",
    "    Returns an RBF embedding of `torch.Tensor` `D` along a new axis=-1.\n",
    "    That is, if `D` has shape [...dims], then the returned tensor will have\n",
    "    shape [...dims, D_count].\n",
    "    '''\n",
    "    D_mu = torch.linspace(D_min, D_max, D_count, device=device)\n",
    "    D_mu = D_mu.view([1, -1])\n",
    "    D_sigma = (D_max - D_min) / D_count\n",
    "    D_expand = torch.unsqueeze(D, -1)\n",
    "\n",
    "    RBF = torch.exp(-((D_expand - D_mu) / D_sigma) ** 2)\n",
    "    return RBF\n",
    "\n",
    "\n",
    "def _positional_embeddings(edge_index, \n",
    "                            num_embeddings=None,\n",
    "                            period_range=[2, 1000], device='cpu'):\n",
    "    # From https://github.com/jingraham/neurips19-graph-protein-design\n",
    "    num_embeddings = 16#num_embeddings or self.num_positional_embeddings\n",
    "    d = edge_index[0] - edge_index[1]\n",
    "    \n",
    "    frequency = torch.exp(\n",
    "        torch.arange(0, num_embeddings, 2, dtype=torch.float32, device=device)\n",
    "        * -(np.log(10000.0) / num_embeddings)\n",
    "    )\n",
    "    angles = d.unsqueeze(-1) * frequency\n",
    "    E = torch.cat((torch.cos(angles), torch.sin(angles)), -1)\n",
    "    return E\n",
    "\n",
    "\n",
    "\n",
    "def make_edge_features(X_ca, top_k, num_rbf, device):\n",
    "    edge_index = torch_cluster.knn_graph(X_ca, k=top_k)\n",
    "    E_vectors = X_ca[edge_index[0]] - X_ca[edge_index[1]]\n",
    "    pos_embeddings = _positional_embeddings(edge_index, device=device)\n",
    "    \n",
    "    rbf = _rbf(E_vectors.norm(dim=-1), D_count=num_rbf, device=device)\n",
    "            \n",
    "    edge_s = torch.cat([rbf, pos_embeddings], dim=-1)\n",
    "    edge_v = _normalize(E_vectors).unsqueeze(-2)\n",
    "            \n",
    "    edge_s, edge_v = map(torch.nan_to_num,\n",
    "                    (edge_s, edge_v))\n",
    "    \n",
    "    return edge_index, (edge_s, edge_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a45ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=determine_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f7487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "if temporal_model == 'tcn':\n",
    "    tcn.eval()\n",
    "else:\n",
    "    transformer.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "all_latents = []\n",
    "all_og_pred = []\n",
    "all_forecasted_pred = []\n",
    "all_og_starts = []\n",
    "first = True \n",
    "previous_coord = None \n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        print(\"batch: \", batch_idx)\n",
    "        B = batch.num_graphs\n",
    "        N = batch.x.size(0) //B\n",
    "        print(\"B:\", B)\n",
    "        \n",
    "        global_start = test_seq_dataset.get_global_start(batch_idx)\n",
    "        global_indices = list(range(global_start, global_start + B))\n",
    "\n",
    "        print(\"  Index start: \", global_indices[0], \" index end: \", global_indices[-1])\n",
    "\n",
    "        matched_local_indices = [\n",
    "            local_i for local_i, g_idx in enumerate(global_indices) if g_idx in forecasted_latents.keys()\n",
    "        ]\n",
    "\n",
    "        if not matched_local_indices:\n",
    "            continue\n",
    "\n",
    "        fore_preds = torch.empty(len(matched_local_indices), N, 3)\n",
    "        og_preds = torch.empty(len(matched_local_indices), N, 3)\n",
    "        og_starts = torch.empty(len(matched_local_indices), N, 3)\n",
    "\n",
    "        print(f\"Processing batch {batch_idx}, global_start={global_start}\")\n",
    "        \n",
    "        batch = batch.to(device)\n",
    "        nodes = (batch.node_s, batch.node_v)\n",
    "        edgesOG = (batch.edge_s, batch.edge_v)\n",
    "\n",
    "        z = encoder(nodes, batch.edge_index, edgesOG)\n",
    "        z_seq = torch.stack([z[start:end] for start,end in zip(batch.seq_ptr[:-1], batch.seq_ptr[1:])])\n",
    "        z_seq_out = tcn(z_seq) if temporal_model == 'tcn' else transformer(z_seq)\n",
    "\n",
    "        z_out = torch.cat([seq for seq in z_seq_out], dim=0)\n",
    "        z_out_cpy = z_out.clone()\n",
    "\n",
    "        for ii,local_i in enumerate(matched_local_indices):\n",
    "            \n",
    "            global_i = global_indices[local_i]\n",
    "            z0_global = global_i - 1\n",
    "\n",
    "            forecast_replacement = forecasted_latents[global_i]\n",
    "            og_latent = og_latents[z0_global]\n",
    "            z_out[local_i] = forecast_replacement\n",
    "            if local_i > 0:\n",
    "                z_out_cpy[local_i-1] = og_latent\n",
    "            GT = batch.x.view(B, N, 3)\n",
    "            if first:\n",
    "                print(\"FIRST INDEX: \", global_i)\n",
    "                coords_curr = GT[local_i].clone()\n",
    "                fore_preds[ii,:,:] = coords_curr \n",
    "                first = False\n",
    "            elif ii==0:\n",
    "                print(\"start of new batch!\")\n",
    "                coords_curr = previous_coord \n",
    "            else:\n",
    "                coords_curr = fore_preds[ii-1,:,:] \n",
    "            \n",
    "\n",
    "            edge_index, edges = make_edge_features(coords_curr, top_k=top_k, num_rbf=16,device=device)\n",
    "            og_edge_index, og_edges = make_edge_features(GT[local_i].clone(), top_k=top_k, num_rbf=16,device=device)\n",
    "   \n",
    "            forecast_pred = decoder(torch.unsqueeze(z_out[local_i], dim=0), edge_index, edges)\n",
    "\n",
    "            og_pred = decoder(torch.unsqueeze(z_out_cpy[local_i], dim=0), og_edge_index, og_edges)\n",
    "\n",
    "            og_start = decoder(torch.unsqueeze(z_out_cpy[local_i-1], dim=0), og_edge_index, og_edges)\n",
    "\n",
    "\n",
    "            if ii == len(matched_local_indices)-1:\n",
    "                print(\"END OF CURRENT BATCH. moving on\")\n",
    "                previous_coord = forecast_pred\n",
    "\n",
    "            fore_preds[ii,:,:] = forecast_pred\n",
    "            og_preds[ii,:,:] = og_pred\n",
    "            og_starts[ii,:,:] = og_start\n",
    "        all_forecasted_pred.append(fore_preds)\n",
    "        all_og_pred.append(og_preds)\n",
    "        all_og_starts.append(og_starts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fff84bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasted_pred_coords = torch.cat(all_forecasted_pred, dim=0)\n",
    "\n",
    "og_pred_coords = torch.cat(all_og_pred, dim=0)\n",
    "\n",
    "og_start_coords = torch.cat(all_og_starts, dim=0)\n",
    "\n",
    "print(forecasted_pred_coords.shape)\n",
    "print(og_pred_coords.shape)\n",
    "print(og_start_coords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ae96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_og_starts = og_start_coords\n",
    "all_forecasted_pred = forecasted_pred_coords\n",
    "all_og_pred = og_pred_coords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6963f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = ~torch.isfinite(all_og_pred) \n",
    "coords = torch.where(bad)          # tuple of index tensors, one per dimension\n",
    "# e.g., for 2D:\n",
    "rows, cols, z = coords\n",
    "print(\"ROWS\", rows, \"COLS\", cols, \"z\", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319efad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_frames = [1,100,330,462]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5018cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "for i, frame in enumerate(testing_frames):\n",
    "    ax = fig.add_subplot(2, 2, i+1, projection='3d')\n",
    "    plot_3d(\n",
    "        base_coords=og_start_coords[frame],\n",
    "        forecasted_coords=forecasted_pred_coords[frame],\n",
    "        true_coords=og_pred_coords[frame],\n",
    "        title=f\"Sample {frame}\",\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/better_chain_forecast_a.pdf\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b8d230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece53554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, FFMpegWriter, PillowWriter\n",
    "\n",
    "# Pick the frames you want to animate\n",
    "frames = range(0, 1000)\n",
    "\n",
    "def to_np(a):\n",
    "    # works for torch tensors or numpy arrays\n",
    "    try:\n",
    "        return a.detach().cpu().numpy()\n",
    "    except AttributeError:\n",
    "        return np.asarray(a)\n",
    "\n",
    "# Compute global axis limits so the view doesn't jump\n",
    "def stack_all(coords_src, idx_list):\n",
    "    return np.vstack([to_np(coords_src[i]) for i in idx_list])\n",
    "\n",
    "all_xyz = np.vstack([\n",
    "    stack_all(all_og_starts, frames),\n",
    "    stack_all(all_forecasted_pred, frames),\n",
    "    stack_all(all_og_pred, frames),\n",
    "])\n",
    "rng = all_xyz.max(axis=0) - all_xyz.min(axis=0)\n",
    "pad = 0.05 * (rng + 1e-12)\n",
    "xyz_min = all_xyz.min(axis=0) - pad\n",
    "xyz_max = all_xyz.max(axis=0) + pad\n",
    "\n",
    "# Figure/axes\n",
    "fig = plt.figure(figsize=(12, 10), dpi=300)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlim(xyz_min[0], xyz_max[0])\n",
    "ax.set_ylim(xyz_min[1], xyz_max[1])\n",
    "ax.set_zlim(xyz_min[2], xyz_max[2])\n",
    "ax.set_box_aspect((xyz_max - xyz_min))\n",
    "ax.set_xlabel('x'); ax.set_ylabel('y'); ax.set_zlabel('z')\n",
    "\n",
    "# --- Create artists ONCE (3 lines + 3 scatters) ---\n",
    "# Use your same colors & styles\n",
    "line_b,   = ax.plot([], [], [], marker='o', linestyle=':',  color=colors[0], label='z0')\n",
    "scat_b     = ax.scatter([], [], [], c=colors[0], s=30)\n",
    "\n",
    "line_f,   = ax.plot([], [], [], marker='o', linestyle='-',  color=colors[1], label='Forecasted')\n",
    "scat_f     = ax.scatter([], [], [], c=colors[1], s=30)\n",
    "\n",
    "line_t,   = ax.plot([], [], [], marker='o', linestyle='--', color=colors[2], label='z1')\n",
    "scat_t     = ax.scatter([], [], [], c=colors[2], s=30)\n",
    "\n",
    "ax.legend()\n",
    "artists = [line_b, line_f, line_t, scat_b, scat_f, scat_t]\n",
    "\n",
    "def get_coords(frame):\n",
    "    b = to_np(all_og_starts[frame])\n",
    "    f = to_np(all_forecasted_pred[frame])\n",
    "    t = to_np(all_og_pred[frame])\n",
    "    return b[:,0], b[:,1], b[:,2], f[:,0], f[:,1], f[:,2], t[:,0], t[:,1], t[:,2]\n",
    "\n",
    "def init():\n",
    "    # nothing to clear; just return artists so blitting knows what to draw\n",
    "    ax.set_title(\"Initializing…\")\n",
    "    return artists\n",
    "\n",
    "def update(k):\n",
    "    frame = frames[k]\n",
    "    bx,by,bz, fx,fy,fz, tx,ty,tz = get_coords(frame)\n",
    "\n",
    "    # Update lines\n",
    "    line_b.set_data_3d(bx, by, bz)\n",
    "    line_f.set_data_3d(fx, fy, fz)\n",
    "    line_t.set_data_3d(tx, ty, tz)\n",
    "\n",
    "    # Update scatters (3D scatter uses _offsets3d)\n",
    "    scat_b._offsets3d = (bx, by, bz)\n",
    "    scat_f._offsets3d = (fx, fy, fz)\n",
    "    scat_t._offsets3d = (tx, ty, tz)\n",
    "\n",
    "    ax.set_title(f\"Sample {frame}\")\n",
    "    return artists\n",
    "\n",
    "anim = FuncAnimation(fig, update, init_func=init, frames=len(frames),\n",
    "                     interval=500, blit=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save if you want:\n",
    "anim.save(\"figs/ACTUAL_CHAINEDforecast_vs_truth.mp4\", writer=FFMpegWriter(fps=8, bitrate=5000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6d7877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb72114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "@torch.no_grad()\n",
    "def tf_vs_free_run_gaps_EM(trainer, samples, chained_samples, z0, t0=0, H=100, diffusion_fn=None):\n",
    "    \"\"\"\n",
    "    trainer.EM(base=z0, cond=z0, diffusion_fn=...) -> (1,D) \n",
    "    Compare step-ahead vs chained samples \n",
    "    Returns: (mse_tf, mse_free, gap) each shape (H,)\n",
    "    \"\"\"\n",
    "    \n",
    "    tf_preds = samples \n",
    "\n",
    "    free_preds = torch.cat(chained_samples, dim=0)  # (H,D)\n",
    "    \n",
    "    tgt =z0\n",
    "    print(\"tgt:\", tgt.shape)\n",
    "    print(\"free preds: \", free_preds.shape)\n",
    "    print(\"tf_preds: \", tf_preds.shape)\n",
    "    mse_tf   = ((tf_preds   - tgt)**2).mean(dim=1).detach().cpu().numpy()\n",
    "    mse_free = ((free_preds - tgt)**2).mean(dim=1).detach().cpu().numpy()\n",
    "    gap = mse_free - mse_tf\n",
    "    return mse_tf, mse_free, gap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d695be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_tf, mse_free, gap = tf_vs_free_run_gaps_EM(trainer, samples, chained_samples,z1s, H=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414ae2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE step ahead forecast\", mse_tf.mean())\n",
    "print(\"MSE chain forecast\", mse_free.mean())\n",
    "print(\"Gap btwn\", gap.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1932e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"step ahead MSE (first 5):   \", np.round(mse_tf[:5], 6))\n",
    "print(\"chain MSE (first 5): \", np.round(mse_free[:5], 6))\n",
    "print(\"GAP (first 5):      \", np.round(gap[:5], 6))\n",
    "print(\"GAP median / max:   \", float(np.median(gap)), float(np.max(gap)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019f0cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 1000\n",
    "horizons = np.arange(1, H+1)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(horizons[:500], mse_tf[:500], label='Step Ahead Prediction',c=colors[2],linewidth=2)\n",
    "plt.plot(horizons[:500], mse_free[:500], label='Chained Forecast',c=colors[1],linewidth=2)\n",
    "plt.xlabel(\"Forecast horizon (steps)\")\n",
    "plt.ylabel('MSE')\n",
    "plt.title(\"MSE vs Horizon\")\n",
    "plt.legend()\n",
    "plt.savefig(\"figs/forecast_mse.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7c118a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gvp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
